model_name: "meta-llama/Meta-Llama-3.1-8B-Instruct" # HuggingFace model id
dataset_path: "dataset/Ko_ARC_ver0.3" 
max_seq_length: 512 # 모델이 처리할 수 있는 최대 시퀀스 길이(모델 성능 및 메모리 사용량에 직접적인 영향)
output_dir: "./llama-3.1-korean-8b-hf-20-epoch"
report_to: "wandb" # wandb, tensorboard, mlflow, azure_ml 등
learning_rate: 0.00005 # high: 빠르지만 불안정 / low: 안정적이지만 느리거나 local minima 빠질 우려
lr_scheduler_type: "linear" # constant, linear, cosine, cosine_with_restarts 등
num_train_epochs: 5 # 1 epoch == 전체 데이터셋 1회 학습
per_device_train_batch_size: 2 # 각 GPU or CPU에서 한번에 처리할 학습 데이터 수
per_device_eval_batch_size: 2 # 각 GPU or CPU에서 한번에 처리할 검증 데이터 수
gradient_accumulation_steps: 4 # 배치를 몇 번 반복 후 가중치를 업데이트 할 것인지 / 배치 수 * gradient_accumulation_steps = 가상의 배치 수
optim: "adamw_torch_fused" # 학습 최적화 알고리즘 / GPU 계산 효율 최적화 된 AdamW
logging_steps: 10 # 로그를 기록할 스텝 주기
save_strategy: "epoch" # 모델 체크포인트 저장 전략 / epochs, steps, no
weight_decay: 0.01 # 과적합을 방지하고 모델 일반화 성을 향상
max_grad_norm: 0.5 # gradient clipping의 임계값 / 모든 파라미터의 그래디언트 합 norm
warmup_ratio: 0.03 # 웜업 기법 제어 / 전체 스텝에서 웜업에 사용하는 퍼센트(%)
bf16: true # Brain Float 16 활성화 / float32의 지수 부분은 그대로 유지하면서 가수 부분을 줄인 16비트 형식
tf32: true # NVIDIA의 Tensor Float 32 활성화 / 19비트 부동소수점 형식으로 내부 연산에만 활용
# gradient_checkpointing: true # 그래디언트 체크포인팅 활성화 / 학습 시 메모리 사용 최적화 - 일부 활성화만 저장하고 나머지는 역전파에서 재계산 (use_cache와 동시 사용 불가)
fsdp: "full_shard auto_wrap" # 모델을 여러 GPU에 분산시켜 메모리 효율 높임 / full_shard: 모든 파라미터 분산
fsdp_config: 
  backward_prefetch: "backward_pre" # Backward Prefetch, FSDP에서 역전파 최적화 / 역전파 계산 전 필요 파라미터 미리 로드
  activation_checkpointing: true
  forward_prefetch: "false" # Gradient Checkpointing 시 성능 최적화를 위한 기법 / FSDP에서는 불필요
  use_orig_params: "false" # 원본 파라미터 사용 vs 분할 파라미터 사용 -> 모델이 sharding 된 파라미터만 유지하여 메모리 효율성 극대화